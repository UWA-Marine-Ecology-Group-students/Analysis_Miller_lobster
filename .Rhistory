count<-length%>%
group_by(Sample,Trip,Family,Genus,Species)%>%
summarise(Count=sum(Count))
# Create count (only use if GlobalArchive won't let me upload without count)
count<-length%>%
group_by(CampaignID,Sample,Trip,Family,Genus,Species)%>%
summarise(Count=sum(Count))
for (i in 1:length(campaigns)){
# Metadata
temp.met <- subset(metadata, CampaignID == campaigns[i])
id<-temp.met$CampaignID
# Remove CampaignID from dataframe and then remove all columns where all are NA
temp.met2<-temp.met%>%select(-c(CampaignID))
temp.met3<-temp.met2[,which(unlist(lapply(temp.met2, function(x)!all(is.na(x)))))]
# Make an empty data frame to bring back in any columns that were removed but need to be in the data
columns<-c("Sample","Latitude","Longitude","Date","Time","Location","Status","Site","Depth","Observer","Successful.count","Successful.length","Comment")%>%map_dfr( ~tibble(!!.x := logical() ) )
# Add back in cols that may have been deleted
temp.met4<-bind_rows(columns, temp.met3)
# write file
write.csv(temp.met4, file=paste(unique(id),"_Metadata.csv",sep=""), quote=FALSE,row.names = FALSE,na = "")
# Length
temp.length <- subset(length, CampaignID == campaigns[i])%>%select(-c(CampaignID)) # Remove CampaignID from dataframe
write.csv(temp.length, file=paste(unique(id),"_Length.csv",sep=""), quote=FALSE,row.names = FALSE,na = "") # write file
# Count
temp.count <- subset(count, CampaignID == campaigns[i])%>%select(-c(CampaignID)) # Remove CampaignID from dataframe
write.csv(temp.count, file=paste(unique(id),"_Count.csv",sep=""), quote=FALSE,row.names = FALSE,na = "") # write file
}
# Create count (only use if GlobalArchive won't let me upload without count)
count<-length%>%
group_by(CampaignID,Sample,Trip,Family,Genus,Species)%>%
summarise(Count=sum(Count))%>%
ungroup()
for (i in 1:length(campaigns)){
# Metadata
temp.met <- subset(metadata, CampaignID == campaigns[i])
id<-temp.met$CampaignID
# Remove CampaignID from dataframe and then remove all columns where all are NA
temp.met2<-temp.met%>%select(-c(CampaignID))
temp.met3<-temp.met2[,which(unlist(lapply(temp.met2, function(x)!all(is.na(x)))))]
# Make an empty data frame to bring back in any columns that were removed but need to be in the data
columns<-c("Sample","Latitude","Longitude","Date","Time","Location","Status","Site","Depth","Observer","Successful.count","Successful.length","Comment")%>%map_dfr( ~tibble(!!.x := logical() ) )
# Add back in cols that may have been deleted
temp.met4<-bind_rows(columns, temp.met3)
# write file
write.csv(temp.met4, file=paste(unique(id),"_Metadata.csv",sep=""), quote=FALSE,row.names = FALSE,na = "")
# Length
temp.length <- subset(length, CampaignID == campaigns[i])%>%select(-c(CampaignID)) # Remove CampaignID from dataframe
write.csv(temp.length, file=paste(unique(id),"_Length.csv",sep=""), quote=FALSE,row.names = FALSE,na = "") # write file
# Count
temp.count <- subset(count, CampaignID == campaigns[i])%>%select(-c(CampaignID)) # Remove CampaignID from dataframe
write.csv(temp.count, file=paste(unique(id),"_Count.csv",sep=""), quote=FALSE,row.names = FALSE,na = "") # write file
}
rm(list=ls()) # Clear memory
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
# install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
library(httr)
library(jsonlite)
library(R.utils)
# To connect to GitHub
library(RCurl)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
## Set your working directory ----
working.dir<-dirname(rstudioapi::getActiveDocumentContext()$path) # to directory of current file - or type your own
## Save these directory names to use later----
staging.dir<-paste(working.dir,"Staging",sep="/")
download.dir<-paste(working.dir,"Downloads",sep="/")
tidy.dir<-paste(working.dir,"Tidy data",sep="/")
## Delete Downloads folder ----
# It will delete any data sitting within your 'Downloads' folder
# DO NOT SAVE ANY OTHER FILES IN YOUR DOWNLOADS FILE
# After running this line they will not be recoverable
# This avoids doubling up GlobalArchive files, or including files from other Projects.
setwd(working.dir)
rm(list=ls()) # Clear memory
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
# install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
library(httr)
library(jsonlite)
library(R.utils)
# To connect to GitHub
library(RCurl)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
## Set Study Name ----
# Change this to suit your study name. This will also be the prefix on your final saved files.
study<-"combined.lobster.dat"
## Set your working directory ----
working.dir<-dirname(rstudioapi::getActiveDocumentContext()$path) # to directory of current file - or type your own
## Save these directory names to use later----
staging.dir<-paste(working.dir,"Staging",sep="/")
download.dir<-paste(working.dir,"Downloads",sep="/")
tidy.dir<-paste(working.dir,"Tidy data",sep="/")
## Delete Downloads folder ----
# It will delete any data sitting within your 'Downloads' folder
# DO NOT SAVE ANY OTHER FILES IN YOUR DOWNLOADS FILE
# After running this line they will not be recoverable
# This avoids doubling up GlobalArchive files, or including files from other Projects.
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Downloads"))
dir.create(file.path(working.dir, "Staging"))
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data/Downloads"))
## Set your working directory ----
work.dir=("C:/GitHub/Analysis_Miller_lobster")
## Save these directory names to use later----
staging.dir<-paste(working.dir,"Data/Staging",sep="/")
download.dir<-paste(working.dir,"Data/Downloads",sep="/")
tidy.dir<-paste(working.dir,"Data/Tidy data",sep="/")
## Delete Downloads folder ----
# It will delete any data sitting within your 'Downloads' folder
# DO NOT SAVE ANY OTHER FILES IN YOUR DOWNLOADS FILE
# After running this line they will not be recoverable
# This avoids doubling up GlobalArchive files, or including files from other Projects.
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data/Downloads"))
## Set your working directory ----
work.dir=("C:/GitHub/Analysis_Miller_lobster")
## Set your working directory ----
working.dir=("C:/GitHub/Analysis_Miller_lobster")
## Save these directory names to use later----
staging.dir<-paste(working.dir,"Data/Staging",sep="/")
download.dir<-paste(working.dir,"Data/Downloads",sep="/")
tidy.dir<-paste(working.dir,"Data/Tidy data",sep="/")
## Delete Downloads folder ----
# It will delete any data sitting within your 'Downloads' folder
# DO NOT SAVE ANY OTHER FILES IN YOUR DOWNLOADS FILE
# After running this line they will not be recoverable
# This avoids doubling up GlobalArchive files, or including files from other Projects.
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data/Downloads"))
dir.create(file.path(working.dir, "Data/Staging"))
dir.create(file.path(working.dir, "Data/Tidy data"))
## Query from GlobalArchive----
# Load default values from GlobalArchive ----
source("https://raw.githubusercontent.com/UWAMEGFisheries/GlobalArchive/master/values.R")
# Add your personal API user token ----
API_USER_TOKEN <- "15b4edc7330c2efadff018bcc5fd684fd346fcaef2bf8a7e038e56c3"
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("BIOL4408+Marine+Ecology+field+trip"))
# Add your personal API user token ----
API_USER_TOKEN <- "95933aec4289dd17cfb5e3ceee841e8f3f619f56d5dce96492566401"
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("BIOL4408+Marine+Ecology+field+trip"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("FRDC+Low+Catch+Zone"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,q=query.project("FRDC+Low+Catch+Zone"))
### Secure access to EventMeasure or generic stereo-video annotations from Campaigns, Projects and Collaborations within GlobalArchive
### OBJECTIVES ###
# 1. use an API token to access Projects and Collaborations shared with you.
# 2. securely download any number of Campaigns within a Project
# 3. combine multiple Campaigns into single Metadata, Count and Length files for subsequent validation and data analysis.
### Please forward any updates and improvements to tim.langlois@uwa.edu.au & brooke.gibbons@uwa.edu.au or raise an issue in the "globalarchive-query" GitHub repository
rm(list=ls()) # Clear memory
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
# install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
library(httr)
library(jsonlite)
library(R.utils)
# To connect to GitHub
library(RCurl)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
## Set Study Name ----
# Change this to suit your study name. This will also be the prefix on your final saved files.
study<-"combined.lobster.dat"
## Folder Structure ----
# This script uses one main folder ('working directory')
# Three subfolders will be created within the 'working directory'. They are 'Downloads','Staging' and 'Tidy data'
# The 'Downloads' folder saves files downloaded from GlobalArchive.
# The 'Staging' folder is used to save the combined files (e.g. metadata, maxn or length) NOTE: These initial outputs have not gone through any check (e.g. checks against the life-history sheet)
# **The only folder you will need to create is your working directory**
## Set your working directory ----
working.dir=("C:/GitHub/Analysis_Miller_lobster")
## Save these directory names to use later----
staging.dir<-paste(working.dir,"Data/Staging",sep="/")
download.dir<-paste(working.dir,"Data/Downloads",sep="/")
tidy.dir<-paste(working.dir,"Data/Tidy data",sep="/")
## Delete Downloads folder ----
# It will delete any data sitting within your 'Downloads' folder
# DO NOT SAVE ANY OTHER FILES IN YOUR DOWNLOADS FILE
# After running this line they will not be recoverable
# This avoids doubling up GlobalArchive files, or including files from other Projects.
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data/Downloads"))
dir.create(file.path(working.dir, "Data/Staging"))
dir.create(file.path(working.dir, "Data/Tidy data"))
## Query from GlobalArchive----
# Load default values from GlobalArchive ----
source("https://raw.githubusercontent.com/UWAMEGFisheries/GlobalArchive/master/values.R")
# An API token allows R to communicate with GlobalArchive
# Finding your API token
# 1. Go to GlobalArchive and login.
# 2. On the front page Click 'Data'.
# 3. In the top right corner click on your name, then 'API token'
# 4. Generate an API token and copy it.
# 5. Paste it below
# Set as tims at the moment
# Add your personal API user token ----
API_USER_TOKEN <- "95933aec4289dd17cfb5e3ceee841e8f3f619f56d5dce96492566401"
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,q=query.project("FRDC+Low+Catch+Zone"))
# Add your personal API user token ----
API_USER_TOKEN <- "56a8062124c909d57af1d21a0d011a0427d1f0ac05c3d54a8600169f"
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("FRDC+Low+Catch+Zone"))
# Add your personal API user token ----
API_USER_TOKEN <- "993ba5c4267b9f8cd21de73b0434c95bc72f518a4f6e725226986022"
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("FRDC+Low+Catch+Zone"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("Pilbara+Marine+Conservation+Partnership"))
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data/Downloads"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("FRDC+low+catch+zone"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.pattern("%Trapping%"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.pattern("%Trapping%"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("FRDC+low+catch+zone"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("FRDC+low+catch+zone"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("%FRDC%"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.pattern("%Masters%"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.campaign("2018-05_Dongara.Millers.Masters_Trapping"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.campaign("2018-05_Bay-of-rest_stereoBRUVs"))
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q='{"filters":[{"name":"name","op":"eq","val":"2018-05_Dongara.Millers.Masters_Trapping"}]}')
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q='{"filters":[{"name":"name","op":"eq","val":"2018-01_Rottnest.sanctuaries_SVC"}]}')
rm(list=ls()) # Clear memory
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
# install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
library(httr)
library(jsonlite)
library(R.utils)
# To connect to GitHub
library(RCurl)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
## Set Study Name ----
# Change this to suit your study name. This will also be the prefix on your final saved files.
study<-"combined.lobster.dat"
## Set your working directory ----
working.dir=("C:/GitHub/Analysis_Miller_lobster")
## Save these directory names to use later----
staging.dir<-paste(working.dir,"Data/Staging",sep="/")
download.dir<-paste(working.dir,"Data/Downloads",sep="/")
tidy.dir<-paste(working.dir,"Data/Tidy data",sep="/")
## Delete Downloads folder ----
# It will delete any data sitting within your 'Downloads' folder
# DO NOT SAVE ANY OTHER FILES IN YOUR DOWNLOADS FILE
# After running this line they will not be recoverable
# This avoids doubling up GlobalArchive files, or including files from other Projects.
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data/Downloads"))
dir.create(file.path(working.dir, "Data/Staging"))
dir.create(file.path(working.dir, "Data/Tidy data"))
## Query from GlobalArchive----
# Load default values from GlobalArchive ----
source("https://raw.githubusercontent.com/UWAMEGFisheries/GlobalArchive/master/values.R")
# Add your personal API user token ----
API_USER_TOKEN <- "993ba5c4267b9f8cd21de73b0434c95bc72f518a4f6e725226986022"
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("FRDC+low+catch+zone"))
# Combine all downloaded data----
## Metadata files ----
metadata <-list.files.GA("Metadata.csv")%>%
purrr::map_df(~read_files_csv(.))%>%
glimpse()
## Count fles ----
count <-list.files.GA("Count.csv")%>%
purrr::map_df(~read_files_csv(.))%>%
glimpse()
## Length files ----
length <-list.files.GA("Length.csv")%>%
purrr::map_df(~read_files_csv(.))%>%
glimpse()
View(length)
### Secure access to EventMeasure or generic stereo-video annotations from Campaigns, Projects and Collaborations within GlobalArchive
### OBJECTIVES ###
# 1. use an API token to access Projects and Collaborations shared with you.
# 2. securely download any number of Campaigns within a Project
# 3. combine multiple Campaigns into single Metadata, Count and Length files for subsequent validation and data analysis.
### Please forward any updates and improvements to tim.langlois@uwa.edu.au & brooke.gibbons@uwa.edu.au or raise an issue in the "globalarchive-query" GitHub repository
rm(list=ls()) # Clear memory
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
# install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
library(httr)
library(jsonlite)
library(R.utils)
# To connect to GitHub
library(RCurl)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
## Set Study Name ----
# Change this to suit your study name. This will also be the prefix on your final saved files.
study<-"combined.lobster.dat"
## Folder Structure ----
# This script uses one main folder ('working directory')
# Three subfolders will be created within the 'working directory'. They are 'Downloads','Staging' and 'Tidy data'
# The 'Downloads' folder saves files downloaded from GlobalArchive.
# The 'Staging' folder is used to save the combined files (e.g. metadata, maxn or length) NOTE: These initial outputs have not gone through any check (e.g. checks against the life-history sheet)
# **The only folder you will need to create is your working directory**
## Set your working directory ----
working.dir=("C:/GitHub/Analysis_Miller_lobster")
## Save these directory names to use later----
data.dir<-paste(working.dir,"Data",sep="/")
staging.dir<-paste(data.dir,"Staging",sep="/")
download.dir<-paste(data.dir,"Downloads",sep="/")
tidy.dir<-paste(data.dir,"Tidy data",sep="/")
## Delete Downloads folder ----
# It will delete any data sitting within your 'Downloads' folder
# DO NOT SAVE ANY OTHER FILES IN YOUR DOWNLOADS FILE
# After running this line they will not be recoverable
# This avoids doubling up GlobalArchive files, or including files from other Projects.
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data"))
dir.create(file.path(data.dir, "Downloads"))
dir.create(file.path(data.dir, "Staging"))
dir.create(file.path(data.dir, "Tidy data"))
rm(list=ls()) # Clear memory
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
# install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
library(httr)
library(jsonlite)
library(R.utils)
# To connect to GitHub
library(RCurl)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
## Set Study Name ----
# Change this to suit your study name. This will also be the prefix on your final saved files.
study<-"combined.lobster.dat"
## Set your working directory ----
working.dir=("C:/GitHub/Analysis_Miller_lobster")
## Save these directory names to use later----
data.dir<-paste(working.dir,"Data",sep="/")
staging.dir<-paste(data.dir,"Staging",sep="/")
download.dir<-paste(data.dir,"Downloads",sep="/")
tidy.dir<-paste(data.dir,"Tidy data",sep="/")
## Delete Downloads folder ----
# It will delete any data sitting within your 'Downloads' folder
# DO NOT SAVE ANY OTHER FILES IN YOUR DOWNLOADS FILE
# After running this line they will not be recoverable
# This avoids doubling up GlobalArchive files, or including files from other Projects.
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data"))
dir.create(file.path(data.dir, "Downloads"))
dir.create(file.path(data.dir, "Staging"))
dir.create(file.path(data.dir, "Tidy data"))
## Query from GlobalArchive----
# Load default values from GlobalArchive ----
source("https://raw.githubusercontent.com/UWAMEGFisheries/GlobalArchive/master/values.R")
# Add your personal API user token ----
API_USER_TOKEN <- "993ba5c4267b9f8cd21de73b0434c95bc72f518a4f6e725226986022"
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("FRDC+low+catch+zone"))
### Secure access to EventMeasure or generic stereo-video annotations from Campaigns, Projects and Collaborations within GlobalArchive
### OBJECTIVES ###
# 1. use an API token to access Projects and Collaborations shared with you.
# 2. securely download any number of Campaigns within a Project
# 3. combine multiple Campaigns into single Metadata, Count and Length files for subsequent validation and data analysis.
### Please forward any updates and improvements to tim.langlois@uwa.edu.au & brooke.gibbons@uwa.edu.au or raise an issue in the "globalarchive-query" GitHub repository
rm(list=ls()) # Clear memory
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
# install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
library(httr)
library(jsonlite)
library(R.utils)
# To connect to GitHub
library(RCurl)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
## Set Study Name ----
# Change this to suit your study name. This will also be the prefix on your final saved files.
study<-"combined.lobster.dat"
## Folder Structure ----
# This script uses one main folder ('working directory')
# Three subfolders will be created within the 'Data' folder within your working directory. They are 'Downloads','Staging' and 'Tidy data'
# The 'Downloads' folder saves files downloaded from GlobalArchive.
# The 'Staging' folder is used to save the combined files (e.g. metadata, maxn or length) NOTE: These initial outputs have not gone through any check (e.g. checks against the life-history sheet)
# **The only folder you will need to create is your working directory**
## Set your working directory ----
working.dir=("C:/GitHub/Analysis_Miller_lobster")
## Save these directory names to use later----
data.dir<-paste(working.dir,"Data",sep="/")
staging.dir<-paste(data.dir,"Staging",sep="/")
download.dir<-paste(data.dir,"Downloads",sep="/")
tidy.dir<-paste(data.dir,"Tidy data",sep="/")
## Delete Downloads folder ----
# It will delete any data sitting within your 'Downloads' folder
# DO NOT SAVE ANY OTHER FILES IN YOUR DOWNLOADS FILE
# After running this line they will not be recoverable
# This avoids doubling up GlobalArchive files, or including files from other Projects.
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data"))
dir.create(file.path(data.dir, "Downloads"))
dir.create(file.path(data.dir, "Staging"))
dir.create(file.path(data.dir, "Tidy data"))
## Query from GlobalArchive----
# Load default values from GlobalArchive ----
source("https://raw.githubusercontent.com/UWAMEGFisheries/GlobalArchive/master/values.R")
# An API token allows R to communicate with GlobalArchive
# Finding your API token
# 1. Go to GlobalArchive and login.
# 2. On the front page Click 'Data'.
# 3. In the top right corner click on your name, then 'API token'
# 4. Generate an API token and copy it.
# 5. Paste it below
# Set as tims at the moment
# Add your personal API user token ----
API_USER_TOKEN <- "993ba5c4267b9f8cd21de73b0434c95bc72f518a4f6e725226986022"
# Set up your query ----
## Download data ----
# These files will be saved in the 'Downloads' folder within your working directory folder
# NOTE: change any spaces in the project name to '+'
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object,
q=query.project("FRDC+low+catch+zone"))
# Combine all downloaded data----
## Metadata files ----
metadata <-list.files.GA("Metadata.csv")%>%
purrr::map_df(~read_files_csv(.))%>%
glimpse()
## Length files ----
length <-list.files.GA("Length.csv")%>%
purrr::map_df(~read_files_csv(.))%>%
glimpse()
## Save metadata, count and length files ----
setwd(staging.dir)
write.csv(metadata,paste(study,"metadata.csv",sep="_"),row.names = FALSE)
write.csv(count,paste(study,"count.csv",sep="_"),row.names = FALSE)
write.csv(length,paste(study,"length.csv",sep="_"),row.names = FALSE)
write.csv(complete.count,paste(study,"complete.count.csv",sep="_"),row.names = FALSE)
