library(lubridate)
#Checks and fixes
data$Lat<-ifelse(data$Lat>0, data$Lat*-1, data$Lat) #convert to negative lats if not already
if(!("Lat" %in% colnames(data))){print("Need a column called Lat in decimal degrees")}
if(!("Long" %in% colnames(data))){print("Need a column called Long in decimal degrees")}
if(!("Date" %in% colnames(data))){print("Need a column called Date in date fromat using as.Date")}
if(is.Date(data$Date)==F) {print("Convert your dates using as.Date")}
rerddap::info(datasetid = "ncdc_oisst_v2_avhrr_by_time_zlev_lat_lon", url = "https://www.ncei.noaa.gov/erddap/")
OISST_sub <- function(times){ #function to download sst data from noaa
oisst_res <- griddap(x = "ncdc_oisst_v2_avhrr_by_time_zlev_lat_lon",
url = "https://www.ncei.noaa.gov/erddap/",
time = times,
depth = c(0, 0),
latitude = c(min(data$Lat)-0.25, max(data$Lat)+0.25),
longitude = c(min(data$Long)-0.25, max(data$Long)+0.25),
fields = "sst")
}
OISST_prep <- function(nc_file){ #function for converting formats
# Open the NetCDF connection
nc <- nc_open(nc_file$summary$filename)
# Extract the SST values and add the lon/lat/time dimension names
res <- ncvar_get(nc, varid = "sst")
dimnames(res) <- list(lon = nc$dim$longitude$vals,
lat = nc$dim$latitude$vals,
t = nc$dim$time$vals)
# Convert the data into a 'long' dataframe for use in the 'tidyverse' ecosystem
res <- as.data.frame(reshape2::melt(res, value.name = "temp"), row.names = NULL) %>%
mutate(t = as.Date(as.POSIXct(t, origin = "1970-01-01 00:00:00")),
temp = round(temp, 2))
# Close the NetCDF connection and finish
nc_close(nc)
return(res)
}
#put
data$years<-format(data$Date,"%Y")
years<-unique(data$years)
for (i in 1:length(years)){ #loop to get start and end dates
start<-paste0(years[i], "-01-01T00:00:00Z")
end<-paste0(years[i], "-12-31T00:00:00Z")
temp <- OISST_sub(c(start, end)) #download data
temp_prepped <- OISST_prep(temp) #prep data
if(i==1){OISST_all<-temp_prepped}else{
OISST_all <- rbind(OISST_all, temp_prepped)}#bind data
} #this downloads the data for interesting years and geographic range and binds it all together
#retrieve unique spatial points for each data set
sst_points <- unique(OISST_all[,c("lon", "lat")])
data_points<- unique(data[ ,c("Long", "Lat", "SiteNo")])
#calculate pairwise distances
d <- distm(data_points[ ,c("Long", "Lat")], sst_points[ ,c("lon", "lat")], fun=distGeo)
row.names(d)<-data_points$SiteNo
#attach lats and longs of four closest sst locations
first<-sst_points[apply(d, 1, function(x) order(x, decreasing=F)[1]),] ; colnames(first)<-paste0(colnames(first),".1" )
second<-sst_points[apply(d, 1, function(x) order(x, decreasing=F)[2]),] ; colnames(second)<-paste0(colnames(second),".2" )
third<-sst_points[apply(d, 1, function(x) order(x, decreasing=F)[3]),] ; colnames(third)<-paste0(colnames(third),".3" )
fourth<-sst_points[apply(d, 1, function(x) order(x, decreasing=F)[4]),] ; colnames(fourth)<-paste0(colnames(fourth),".4" )
data_points<-cbind(data_points, first, second, third, fourth)
#merge in lats and longs for sst into main dataset
data$SiteNo<-as.character(data$SiteNo) ; data_points$SiteNo<-as.character(data_points$SiteNo)
data %<>% left_join(.,data_points[, 3:ncol(data_points)], c("SiteNo" = "SiteNo"))
data$ID<-paste0(data$lon.1,data$lat.1,data$Date)
data$ID2<-paste0(data$lon.2,data$lat.2,data$Date)
data$ID3<-paste0(data$lon.3,data$lat.3,data$Date)
data$ID4<-paste0(data$lon.4,data$lat.4,data$Date)
OISST_all$ID<-paste0(OISST_all$lon,OISST_all$lat,OISST_all$t)
data$sst<-OISST_all$temp[match(data$ID, OISST_all$ID)] #check first
data$sst<-ifelse(is.na(data$sst),OISST_all$temp[match(data$ID2, OISST_all$ID)], data$sst ) #check second
data$sst<-ifelse(is.na(data$sst),OISST_all$temp[match(data$ID3, OISST_all$ID)], data$sst ) #check third
data$sst<-ifelse(is.na(data$sst),OISST_all$temp[match(data$ID4, OISST_all$ID)], data$sst ) #check fourth
if(anyNA(data$sst)){print("Email matt and say: There are a lot of sst NAs. You need to increase the number of nearest points to search")}
return(data$sst)
} #Matts Function for sst data
# # For Rstudio Server
options(httr_oob_default=TRUE)
dat.length<-read.csv("length.csv")%>%
mutate(ID=1:nrow(.))%>% #Need so Total damage line below is summed per row, not total
group_by(ID)%>%
dplyr::mutate(Total.damage=sum(Damage.old.a+Damage.old.l+Damage.new.a+Damage.new.l,na.rm = TRUE))%>%
glimpse()
#Check
length(unique(dat.length$Tag.number)) #9216
length(dat.length$Carapace.length) #13657
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
#select(Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
filter(!Source=="")
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
#select(Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
#filter(!Source=="")
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
glimpse()
unique(dat.pot$Source)
View(dat.pot)
View(dat.pot)
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
filter(!Source=="fisher-returns")
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
filter(!Source=="fisher-returns")%>%
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
glimpse()
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
filter(!Source=="fisher-returns")%>%
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
glimpse()
unique(dat.pot$Source)
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
filter(!Source=="fisher-returns")%>%
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Source,Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
glimpse()
unique(dat.pot$Source)
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter((!Source%in%c("fisher-returns")))%>%
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Source,Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
glimpse()
unique(dat.pot$Source)
help('droplevels')
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter((!Source%in%c("fisher-returns")))%>%
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Source,Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
glimpse()
unique(dat.pot$Source)
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
#filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter((!Source%in%c("fisher-returns")))%>%
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Source,Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
glimpse()
unique(dat.pot$Source)
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter((!Source%in%c("fisher-returns")))%>%
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Source,Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
glimpse()
unique(dat.pot$Source)
#Join Length and Pot data
names(length)
#Join Length and Pot data
glimpse(length)
#Join Length and Pot data
glimpse(dat.length)
#Join Length and Pot data
names(dat.length)
names(dat.pot)
dat.all<- left_join(dat.pot, dat.length, by=Source)
dat.all<- left_join(dat.pot, dat.length, by="Source")
dat.all<- left_join(dat.pot, dat.length, by="Source")%>%
glimpse()
dat.all<- left_join(dat.length,dat.pot, by="Source")%>%
glimpse()
dat.all<- left_join(dat.length,dat.pot)%>%
glimpse()
#Join Length and Pot data
glimpse(dat.length)
glimpse(dat.pot)
dat.all<- left_join(dat.pot, dat.length, by=Sample)%>%
glimpse()
dat.all<- left_join(dat.pot, dat.length, by="Sample")%>%
glimpse()
dat.all<- inner_join(dat.pot, dat.length, by="Sample")%>%
glimpse()
dat.all<- inner_join(dat.length,pot.dat, by="Sample")%>%
glimpse()
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Source, Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
glimpse()
dat.all<- dplyr::left_join(dat.length, by="Sample")%>%
glimpse()
dat.all<- dplyr::left_join(dat.length,dat.pot, by="Sample")%>%
glimpse()
#Join Length and Pot data
glimpse(dat.length) #14,503
glimpse(dat.pot) #1,782
unique(dat.pot$Pwo)
dat.pot.1<-dat.pot%>%
select(sample)%>%
glimpse()
dat.pot.1<-dat.pot%>%
select(Sample)%>%
glimpse()
dat.all<- dplyr::left_join(dat.length,dat.pot, by="Sample")%>%
glimpse()
dat.all<- dplyr::left_join(dat.length,dat.pot.1, by="Sample")%>%
glimpse()
unique(dat.length$Location)
glimpse(dat.length)
dat.length<-read.csv("length.csv")%>%
mutate(ID=1:nrow(.))%>% #Need so Total damage line below is summed per row, not total
group_by(ID)%>%
dplyr::mutate(Total.damage=sum(Damage.old.a+Damage.old.l+Damage.new.a+Damage.new.l,na.rm = TRUE))%>%
select(Sample, Tag.number, Carapace.length, Sex, Colour, Count)%>%
glimpse()
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Source, Trip, Date, Location, Site, Pot.number, Day.pull, Pwo, Longitude, Latitude, Sample)%>%
glimpse()
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Date, Location, Site,Day.pull, Sample)%>%
glimpse()
dat.all<- dplyr::left_join(dat.length,dat.pot, by="Sample")%>%
glimpse()
unique(dat.pot$Sample)
unique(dat.length$Sample)
dat.all<- dplyr::left_join(dat.length,dat.pot, by="Sample")%>%
glimpse()
dat.all<- dplyr::left_join(dat.pot,dat.length)%>%
glimpse()
#Join Length and Pot data----
glimpse(dat.length) #14,503
glimpse(dat.pot) #1,782
dat.all<- dplyr::left_join(dat.length,dat.pot)%>%
glimpse()
#Join Length and Pot data----
glimpse(dat.length) #14,503
dat.all<- dplyr::left_join(dat.length,dat.pot)%>%
glimpse()
#Join Length and Pot data----
glimpse(dat.length) #14,503
dat.length<-dat.length%>%
mutate(Tag.number=as.character(Tag.number))%>%
mutate(Sample=as.character(Sample))
dat.pot<-dat.length%>%
mutate(Sample=as.character(Sample))
dat.all<- dplyr::left_join(dat.length,dat.pot)%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length,dat.pot)%>%
glimpse()
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Date, Location, Site,Day.pull, Sample)%>%
glimpse()
dat.pot<-dat.pot%>%
mutate(Sample=as.character(Sample))
dat.length<-read.csv("length.csv")%>%
mutate(ID=1:nrow(.))%>% #Need so Total damage line below is summed per row, not total
group_by(ID)%>%
dplyr::mutate(Total.damage=sum(Damage.old.a+Damage.old.l+Damage.new.a+Damage.new.l,na.rm = TRUE))%>%
select(Sample, Tag.number, Carapace.length, Sex, Colour, Count)%>%
glimpse()
#Check
length(unique(dat.length$Tag.number)) #9716
length(dat.length$Carapace.length) #14503
glimpse(dat.length)
unique(dat.length$Location)
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Date, Location, Site,Day.pull, Sample)%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length,dat.pot)%>%
glimpse()
dat.legal <-dat.all%>%
mutate(sizeclass= ifelse(Carapace.length>=76.0,"Legal", "Sublegal"))%>%
filter(!is.na(Carapace.length))%>%
glimpse()
unique(dat.pot$Sample)
unique(dat.length$Sample)
length(dat.all)
dat.legal <-dat.all%>%
mutate(sizeclass= ifelse(Carapace.length>=76.0,"Legal", "Sublegal"))%>%
filter(!is.na(Carapace.length))%>%
glimpse()
sum.legal <-dat.legal%>%
group_by(trip.day.trap, sizeclass)%>%
dplyr::summarise(Count=sum(Count))%>%
ungroup()%>%
distinct()%>%
glimpse()
sum.legal <-dat.legal%>%
group_by(Sample, sizeclass)%>%
dplyr::summarise(Count=sum(Count))%>%
ungroup()%>%
distinct()%>%
glimpse()
unique(dat.all$Sample)
View(sum.legal)
View(sum.legal)
#FIND SUM PER POT----
sum.dat <-dat.all%>%
group_by(Sample)%>%
dplyr::summarise(Count=sum(Count))%>%
ungroup()%>%
mutate(sizeclass="All")%>%
distinct()%>%
bind_rows(.,sum.legal)%>%
arrange(Sample)%>%
glimpse()
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Date, Location, Site,Day.pull, Sample, Longitude, Latitude )%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length,dat.pot)%>%
glimpse()
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Date, Location, Site,Day.pull, Sample, Longitude, Latitude )%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length,dat.pot)%>%
glimpse()
dat.all<- dplyr::(dat.pot, dat.length)%>%
glimpse()
dat.all<- dplyr::semi_join(dat.pot,dat.length)%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length, dat.pot)%>%
glimpse()
dat.lat.long<-dat.pot%>%
select(Sample, Latitude. Longitude)%>%
glimpse()
dat.lat.long<-dat.pot%>%
select(Sample, Latitude, Longitude)%>%
glimpse()
dat.all%<>%
left_join(dat.lat.long)%>%
glimpse()
dat.all%<>%
semi_join(dat.lat.long)%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length, dat.pot)%>%
glimpse()
dat.all.1<-dplyr::left_join(dat.all, dat.lat.long)
dat.all.1<-dplyr::left_join(dat.all, dat.lat.long)%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length, dat.pot)%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length, dat.pot)%>%
left_join(.,dat.pot)%>%
glimpse()
glimpse(dat.pot) #1,782
dat.all<- dplyr::semi_join(dat.length, dat.pot)
dat.all<- dplyr::semi_join(dat.length, dat.pot)%>%
left_join(.,dat.pot)%>%
glimpse()
View(dat.all)
dat.all<-dplyr::inner_join(dat.length, dat.pot)%>%
glimpse()
dat.length<-read.csv("length.csv")%>%
mutate(ID=1:nrow(.))%>% #Need so Total damage line below is summed per row, not total
group_by(ID)%>%
dplyr::mutate(Total.damage=sum(Damage.old.a+Damage.old.l+Damage.new.a+Damage.new.l,na.rm = TRUE))%>%
select(Sample, Tag.number, Carapace.length, Sex, Colour, Count)%>%
ungroup()%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length, dat.pot)%>%
left_join(.,dat.pot)%>%
glimpse()
dat.all<-dplyr::inner_join(dat.length, dat.pot)%>%
glimpse()
dat.length<-read.csv("length.csv")%>%
mutate(ID=1:nrow(.))%>% #Need so Total damage line below is summed per row, not total
group_by(ID)%>%
dplyr::mutate(Total.damage=sum(Damage.old.a+Damage.old.l+Damage.new.a+Damage.new.l,na.rm = TRUE))%>%
select(Sample, Tag.number, Carapace.length, Sex, Colour, Count)%>%
ungroup()%>%
glimpse()
dat.length<-read.csv("length.csv")%>%
mutate(ID=1:nrow(.))%>% #Need so Total damage line below is summed per row, not total
group_by(ID)%>%
dplyr::mutate(Total.damage=sum(Damage.old.a+Damage.old.l+Damage.new.a+Damage.new.l,na.rm = TRUE))%>%
ungroup()%>%
select(Sample, Tag.number, Carapace.length, Sex, Colour, Count)%>%
glimpse()
#Import Pot data
dat.pot<-read.csv("metadata.csv")%>%
filter(!Location=="Rivermouth")%>% #Removes NAs from Fishers as well. good.
dplyr::filter(!Pwo%in%c("1", "2", "X"))%>% #Removed 148 pots with Occy's.
select(Date, Location, Site,Day.pull, Sample, Longitude, Latitude )%>%
glimpse()
dat.all<- dplyr::semi_join(dat.length, dat.pot)%>%
left_join(.,dat.pot)%>%
glimpse()
dat.all<-dplyr::inner_join(dat.length, dat.pot)%>%
glimpse()
View(dat.all)
View(dat.length)
View(dat.pot)
install.packages("R.utils")
rm(list=ls()) # Clear memory
## Load Libraries ----
# To connect to GlobalArchive
library(devtools)
# install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
library(GlobalArchive)
library(httr)
library(jsonlite)
library(R.utils)
# To connect to GitHub
library(RCurl)
# To tidy data
library(plyr)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
library(lubridate)
# For googlesheets
library(googlesheets)
## Set your working directory ----
working.dir=("C:/GitHub/Analysis_Miller_lobster")
work.dir=("Z:/Analysis_Miller_lobster") # FOr Ash's laptop using Git
## Save these directory names to use later----
data.dir<-paste(working.dir,"Data",sep="/")
download.dir<-paste(data.dir,"Downloads",sep="/")
tidy.dir<-paste(data.dir,"Tidy data",sep="/")
## Delete Downloads folder ----
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data"))
dir.create(file.path(data.dir, "Downloads"))
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data"))
dir.create(file.path(data.dir, "Downloads"))
dir.create(file.path(data.dir, "Staging"))
dir.create(file.path(data.dir, "Tidy data"))
work.dir=("Z:/Analysis_Miller_lobster") # FOr Ash's laptop using Git
## Save these directory names to use later----
data.dir<-paste(working.dir,"Data",sep="/")
download.dir<-paste(data.dir,"Downloads",sep="/")
tidy.dir<-paste(data.dir,"Tidy data",sep="/")
## Delete Downloads folder ----
setwd(working.dir)
setwd("Z:/Analysis_Miller_lobster")
working.dir=("Z:/Analysis_Miller_lobster") # FOr Ash's laptop using Git
## Save these directory names to use later----
data.dir<-paste(working.dir,"Data",sep="/")
download.dir<-paste(data.dir,"Downloads",sep="/")
tidy.dir<-paste(data.dir,"Tidy data",sep="/")
## Delete Downloads folder ----
setwd(working.dir)
unlink(download.dir, recursive=TRUE)
## Create Downloads, Staging and Tidy data folders ----
dir.create(file.path(working.dir, "Data"))
dir.create(file.path(data.dir, "Downloads"))
dir.create(file.path(data.dir, "Staging"))
dir.create(file.path(data.dir, "Downloads"))
dir.create(file.path(data.dir, "Tidy data"))
## Query from GlobalArchive----
# Load default values from GlobalArchive ----
source("https://raw.githubusercontent.com/UWAMEGFisheries/GlobalArchive/master/values.R")
# Set as tims at the moment
# Add your personal API user token ----
API_USER_TOKEN <- "993ba5c4267b9f8cd21de73b0434c95bc72f518a4f6e725226986022"
# Set up your query ----
## Download data ----
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object, q=ga.query.project("FRDC+low+catch+zone"))
library(GlobalArchive)
install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
# Set up your query ----
## Download data ----
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object, q=ga.query.project("FRDC+low+catch+zone"))
library(GlobalArchive)
# Set up your query ----
## Download data ----
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object, q=ga.query.project("FRDC+low+catch+zone"))
install_github("UWAMEGFisheries/GlobalArchive") #to check for updates
install_github("UWAMEGFisheries/GlobalArchive",force=TRUE) #to check for updates
library(GlobalArchive)
# Set up your query ----
## Download data ----
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object, q=ga.query.project("FRDC+low+catch+zone"))
library(GlobalArchive)
# Combine all downloaded data----
## Metadata files ----
metadata <-ga.list.files("Metadata.csv")%>%
purrr::map_df(~ga.read.files_csv(.))%>%
dplyr::mutate(Latitude=as.numeric(latitude))%>%
dplyr::mutate(Longitude=as.numeric(longitude))%>%
dplyr::mutate(Trip=as.numeric(trip))%>%
dplyr::mutate(Date=paste((str_sub(date,1,4)),(str_sub(date,5,6)),(str_sub(date,7,8)),sep="-"))%>%
dplyr::mutate(Date=lubridate::as_date(ymd(Date)))%>%
dplyr::select(-c(date,latitude,longitude,trip))%>%
glimpse()
# Set up your query ----
## Download data ----
ga.get.campaign.list(API_USER_TOKEN, process_campaign_object, q=GlobalArchive::ga.query.project("FRDC+low+catch+zone"))
